Using cache found in /Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main
/Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Using cache found in /Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main
Total parameters: 116806593
/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py:158: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
0it [00:00, ?it/s]/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py:172: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
4
  with autocast(dtype=torch.bfloat16):
/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
torch.Size([4, 1024, 768])
gt ( 1st element ): tensor([0.0137, 0.0137, 0.0137,  ..., 0.0538, 0.0538, 0.0528])
pred ( 1st element ): tensor([0.0767, 0.0764, 0.0756,  ..., 0.0757, 0.0719, 0.0668],
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(6569.7510)
pred aggregation ( 1st element ): tensor(17017.4375, grad_fn=<SumBackward1>)
0it [01:46, ?it/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py", line 263, in <module>
    main()
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py", line 260, in main
    train_student()
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py", line 177, in train_student
    scaler.scale(loss).backward()
  File "/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/anaconda3/lib/python3.12/site-packages/wandb/integration/torch/wandb_torch.py", line 276, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))

KeyboardInterrupt
