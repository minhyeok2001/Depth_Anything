Using cache found in C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main
C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main\dinov2\layers\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main\dinov2\layers\attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main\dinov2\layers\block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Using cache found in C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main
Total parameters: 116806593
C:\Users\mhroh\Depth_Anything\src\train.py:172: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
0it [00:00, ?it/s]C:\Users\mhroh\Depth_Anything\src\train.py:186: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2
  with autocast(dtype=torch.bfloat16):
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.0000, 0.0058, 0.0058,  ..., 1.0000, 1.0000, 1.0000], device='cuda:0')
pred ( 1st element ): tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0',
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(45461.7695, device='cuda:0')
pred aggregation ( 1st element ): tensor(59.9922, device='cuda:0', grad_fn=<SumBackward1>)
feature_loss skipped!!
Epoch [1/20], Batch [0], Loss: 3.7907
4it [00:41,  9.85s/it]
2
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.2759, 0.2874, 0.2759,  ..., 0.9655, 0.9655, 0.9655], device='cuda:0')
pred ( 1st element ): tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0',
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(63008.4844, device='cuda:0')
pred aggregation ( 1st element ): tensor(4300.4668, device='cuda:0', grad_fn=<SumBackward1>)
feature_loss skipped!!
2
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.0167, 0.0167, 0.0167,  ..., 0.9000, 0.9000, 0.9000], device='cuda:0')
pred ( 1st element ): tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0',
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(94450.9531, device='cuda:0')
pred aggregation ( 1st element ): tensor(11759.2256, device='cuda:0', grad_fn=<SumBackward1>)
feature_loss skipped!!
2
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.1716, 0.1716, 0.1765,  ..., 0.7843, 0.7843, 0.7843], device='cuda:0')
pred ( 1st element ): tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0',
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(79500.3906, device='cuda:0')
pred aggregation ( 1st element ): tensor(10363.8691, device='cuda:0', grad_fn=<SumBackward1>)
feature_loss skipped!!
2
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.7462, 0.7462, 0.7462,  ..., 0.2893, 0.2893, 0.2893], device='cuda:0')
pred ( 1st element ): tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0',
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(121699.4922, device='cuda:0')
pred aggregation ( 1st element ): tensor(17556.0781, device='cuda:0', grad_fn=<SumBackward1>)
feature_loss skipped!!
2
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
pred ( 1st element ): tensor([0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000], device='cuda:0',
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(82111.5156, device='cuda:0')
pred aggregation ( 1st element ): tensor(21400.2227, device='cuda:0', grad_fn=<SumBackward1>)
2
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.3492, 0.3492, 0.3492,  ..., 0.1587, 0.1667, 0.1984], device='cuda:0')
pred ( 1st element ): tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0',
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(89623.5078, device='cuda:0')
pred aggregation ( 1st element ): tensor(14883.7178, device='cuda:0', grad_fn=<SumBackward1>)
2
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.5988, 0.5988, 0.6049,  ..., 0.7037, 0.7284, 0.8580], device='cuda:0')
pred ( 1st element ): tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0',
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(60105.3555, device='cuda:0')
pred aggregation ( 1st element ): tensor(11848.6816, device='cuda:0', grad_fn=<SumBackward1>)
Traceback (most recent call last):
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 289, in <module>
    main()
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 286, in main
    train_student()
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 192, in train_student
    scaler.step(optimizer)
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\site-packages\torch\amp\grad_scaler.py", line 457, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\site-packages\torch\amp\grad_scaler.py", line 351, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\site-packages\torch\amp\grad_scaler.py", line 351, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
Traceback (most recent call last):
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 289, in <module>
    main()
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 286, in main
    train_student()
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 192, in train_student
    scaler.step(optimizer)
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\site-packages\torch\amp\grad_scaler.py", line 457, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\site-packages\torch\amp\grad_scaler.py", line 351, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\site-packages\torch\amp\grad_scaler.py", line 351, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
