Using cache found in /Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main
/Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Using cache found in /Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main
Total parameters: 116806593
/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py:158: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
0it [00:00, ?it/s]/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py:172: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2
  with autocast(dtype=torch.bfloat16):
/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([2.0206e-10, 2.0206e-10, 2.0206e-10,  ..., 3.4148e-11, 3.3371e-11,
        3.3371e-11])
pred ( 1st element ): tensor([0.0030, 0.0060, 0.0063,  ..., 0.0000, 0.0000, 0.0019],
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(1745.)
pred aggregation ( 1st element ): tensor(2188.3706, grad_fn=<SumBackward1>)
0it [00:18, ?it/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py", line 263, in <module>
    main()
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py", line 260, in main
    train_student()
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py", line 176, in train_student
    loss = loss_module(outputs, targets,len_data=(inputs.shape[0]),disparity=False,frozen_encoder_result=frozen_feature[0], encoder_result=burning_feature)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/loss/loss_student.py", line 150, in forward
    loss_u=(loss_mask1 + loss_mask2)/group_1_pred.shape[-1]
            ~~~~~~~~~~~^~~~~~~~~~~~
RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0
