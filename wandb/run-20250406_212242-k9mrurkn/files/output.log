Using cache found in /Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main
/Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Using cache found in /Users/minhyeokroh/.cache/torch/hub/facebookresearch_dinov2_main
Total parameters: 116806593
/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py:158: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
0it [00:00, ?it/s]/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py:172: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2
  with autocast(dtype=torch.bfloat16):
/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.9632, 0.9632, 0.9632,  ..., 0.0029, 0.0029, 0.0029])
pred ( 1st element ): tensor([0.1330, 0.1418, 0.1424,  ..., 0.1499, 0.1449, 0.1381],
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(26092.0996)
pred aggregation ( 1st element ): tensor(30940.5156, grad_fn=<SumBackward1>)
0it [00:12, ?it/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py", line 263, in <module>
    main()
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py", line 260, in main
    train_student()
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/train.py", line 176, in train_student
    loss = loss_module(outputs, targets,len_data=(inputs.shape[0]),disparity=False,frozen_encoder_result=frozen_feature[0], encoder_result=burning_feature)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/minhyeokroh/PycharmProjects/JupyterProject/models/Depth_Anything/src/loss/loss_student.py", line 139, in forward
    valid_pred_mask2 = pred_mask2_full[(1 - mask).bool()].view(group_2_y[0],-1)    ## 이거 배치가 4니까 그냥 view 4로 가는거야
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: view() received an invalid combination of arguments - got (Tensor, int), but expected one of:
 * (torch.dtype dtype)
 * (tuple of ints size)
