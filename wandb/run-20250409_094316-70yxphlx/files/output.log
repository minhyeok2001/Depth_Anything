Using cache found in C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main
C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main\dinov2\layers\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main\dinov2\layers\attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main\dinov2\layers\block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Using cache found in C:\Users\mhroh/.cache\torch\hub\facebookresearch_dinov2_main
Total parameters: 116806593
C:\Users\mhroh\Depth_Anything\src\train.py:171: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
0it [00:00, ?it/s]C:\Users\mhroh\Depth_Anything\src\train.py:185: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2
  with autocast(dtype=torch.bfloat16):
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.5701, 0.5701, 0.5701,  ..., 0.8224, 0.8224, 0.8224], device='cuda:0')
pred ( 1st element ): tensor([0.1396, 0.1465, 0.1465,  ..., 0.1436, 0.1426, 0.1387], device='cuda:0',
       grad_fn=<SelectBackward0>)
y aggregation ( 1st element ): tensor(167838.7812, device='cuda:0')
pred aggregation ( 1st element ): tensor(31076.4258, device='cuda:0', grad_fn=<SumBackward1>)
feature_loss skipped!!
Epoch [1/20], Batch [0], Loss: 3.4987
1it [00:10, 10.36s/it]
Epoch [1/20] Loss: 3.4987
2
C:\Users\mhroh\Depth_Anything\src\train.py:211: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(dtype=torch.bfloat16):
torch.Size([2, 1024, 768])
gt ( 1st element ): tensor([0.6386, 0.6386, 0.6386,  ..., 0.7028, 0.7028, 0.7068], device='cuda:0')
pred ( 1st element ): tensor([0.1387, 0.1426, 0.1445,  ..., 0.1602, 0.1611, 0.1514], device='cuda:0')
y aggregation ( 1st element ): tensor(128095.3906, device='cuda:0')
pred aggregation ( 1st element ): tensor(30819.9062, device='cuda:0')
feature_loss skipped!!
flat_pred.dtype: torch.float32
pred_hat.dtype: torch.float32
element_1.dtype: torch.bfloat16
Traceback (most recent call last):
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 287, in <module>
    main()
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 284, in main
    train_student()
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 215, in train_student
    outputs = scale_shift_correction(outputs, targets)
  File "C:\Users\mhroh\Depth_Anything\src\utils\metrics.py", line 22, in scale_shift_correction
    h_opt = torch.inverse(element_1) @ element_2
RuntimeError: linalg.inv: Low precision dtypes not supported. Got BFloat16
Traceback (most recent call last):
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "C:\Users\mhroh\anaconda3\envs\dl_env\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 287, in <module>
    main()
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 284, in main
    train_student()
  File "C:\Users\mhroh\Depth_Anything\src\train.py", line 215, in train_student
    outputs = scale_shift_correction(outputs, targets)
  File "C:\Users\mhroh\Depth_Anything\src\utils\metrics.py", line 22, in scale_shift_correction
    h_opt = torch.inverse(element_1) @ element_2
RuntimeError: linalg.inv: Low precision dtypes not supported. Got BFloat16
